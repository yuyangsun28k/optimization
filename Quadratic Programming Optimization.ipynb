{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e7dc78f-0c98-4005-8bd8-b66ebaf3cb81",
   "metadata": {},
   "source": [
    "# Quadratic Optimization\n",
    "\n",
    "## Introduction of Quadratic Optimization\n",
    "Quadratic Optimization is a process to solve mathematical optimization problems. Similar to linear optimization, quadratic optimization has an objective function, decision variables and constraints. The quadratic optimization is a form of non-linear programming with linear constraints. Let's perform a standard quadratic optimization in mathematical symbols:\n",
    "\n",
    "$$\n",
    "\\begin{array}{rc}\n",
    "\\text{minimize:} & \\frac{1}{2}\\mathbf{x}^T \\mathbf{P} \\mathbf{x} + \\mathbf{q}^T \\mathbf{x}\\\\\n",
    "\\text{subject to:} & G \\mathbf{x} \\leq \\mathbf{h} \\\\\n",
    "& A \\mathbf{x} = \\mathbf{b} \\\\\n",
    "& \\mathbf{l} \\mathbf{b} \\leq \\mathbf{x} \\leq \\mathbf{l} \\mathbf{b}\n",
    "\\end{array}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed4a986a-1bed-4de8-a239-88609de769de",
   "metadata": {},
   "source": [
    "The format of quadratic optimization is almost the same as linear optimization. The x is the vector of optimization variables from x1,…,xn. The matrix P and vector q are used to define a quadratic objective function on these x variables, while (G,h) and (A,b) are constraints, with different signs. Usually, when we look at the matrix P, we would make it as a positive definite matrix, where $ \\mathbf{z}^T \\mathbf{P} \\mathbf{z} \\geq 0 $. The matrix P should only have real entries and $\\mathbf{z}$ is a column vector with non-zero numbers and $\\mathbf{z}^T$ is the transpose of $\\mathbf{z}$. In addition, vector inequalities are taken coordinate by coordinate. The decision variables x also have upper and lower constrints in certain situations (Quadratic Programming 2022).\n",
    "\n",
    "Quadratic optimization is widely used in many real-world topics like image/signal processing, financial protfolios, solving complex non-linear programming problems and perform the least-squares method of regression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cbdd6d0-2335-48dd-af69-830093095b2b",
   "metadata": {},
   "source": [
    "## Conversion between Least-Sauqres Method and Quadratic Optimization\n",
    "\n",
    "### Intuition\n",
    "\n",
    "This section will focus on the connections between least-squares method and quadratic optimization. Intuitively, if we take a look at the above objective function of quadratic optimization, the goal is to minimize the objective function. The method least-squares also aims to minimize the residuals for the best-fitting line in the regression problems. From that, we can relate the two and see how a least-square problem can be convert to a quadratic problem.\n",
    "The convertion from least-square to quadratic programming. I follow the steps on the website and it works. (Caron, https://scaron.info/blog/conversion-from-least-squares-to-quadratic-programming.html)\n",
    "\n",
    "### Least-Square Structure\n",
    "\n",
    "If we take a close look at the structure of least-squares estimation and lets define a standard linear least-square problem:\n",
    "$$\n",
    "\\begin{array}{rc}\n",
    "\\text{minimize:} & \\frac{1}{2}||R\\mathbf{x}- \\mathbf{s}||^2_\\mathbf{w} = \\frac{1}{2}(R\\mathbf{x}- \\mathbf{s})^T \\mathbf{w} (R\\mathbf{x}- \\mathbf{s})\\\\\n",
    "\\text{subject to:} & G \\mathbf{x} \\leq \\mathbf{h} \\\\\n",
    "& A \\mathbf{x} = \\mathbf{b} \\\\\n",
    "& \\mathbf{l} \\mathbf{b} \\leq \\mathbf{x} \\leq \\mathbf{l} \\mathbf{b}\n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d4f31a5-251b-4b51-9a48-7bb0df701b4b",
   "metadata": {},
   "source": [
    "Similarly to quadratic optimization, the target of least-squares is to minimize the function and make $R\\mathbf{x}$ close to the prescribed vector $\\mathbf{s}$. Here the weight matrix $\\mathbf{w}$ is positive semi-definite and usually diagonal. Matrix-vector pairs (G,h) and (A,b) are constraints and vector inequalities are taken coordinate by coordinate. The decision variables x also have upper and lower constrints in certain situations.\n",
    "\n",
    "### Conversion\n",
    "Since only the objective functions are differrent, we could build a connection between least-square methods and quadratic optimization and convert the least square method to quadratic form.\n",
    "\n",
    "$$||R\\mathbf{x}- \\mathbf{s}||^2_\\mathbf{w} = (R\\mathbf{x}- \\mathbf{s})^T \\mathbf{w} (R\\mathbf{x}- \\mathbf{s}) \\\\\n",
    "  = \\mathbf{x}^T R^T \\mathbf{w} R\\mathbf{x} - \\mathbf{x}^T R^T \\mathbf{w} \\mathbf{s} - \\mathbf{s}^T \\mathbf{w} R\\mathbf{x} + \\mathbf{s}^T \\mathbf{w} \\mathbf{s}$$\n",
    "  \n",
    "We expend the $||R\\mathbf{x}- \\mathbf{s}||^2_\\mathbf{w}$ into two parts, $(R\\mathbf{x}- \\mathbf{s})^T \\mathbf{w}$ and  $(R\\mathbf{x}- \\mathbf{s})$ due to the property of norm. One of the $(R\\mathbf{x}- \\mathbf{s})$ will take transpose then times $ \\mathbf{w}$. We expend the two parts again and get four different pieces. The constant term $\\mathbf{s}^T \\mathbf{w} \\mathbf{s}$ can be discarded as it cannot affect the optimum of this problem. Notice that the matrix $\\mathbf{w}$ is semi-definite and has the property of $\\mathbf{w}^T = \\mathbf{w}$, we \n",
    "can do some variations to $\\mathbf{s}^T \\mathbf{w} R\\mathbf{x}$ then. The number of $\\mathbf{s}^T \\mathbf{w} R\\mathbf{x}$ equals to its transpose.\n",
    "\n",
    "Now we have:\n",
    "$$\\mathbf{s}^T \\mathbf{w} R\\mathbf{x}= (\\mathbf{s}^T \\mathbf{w} R\\mathbf{x})^T = \\mathbf{x}^T R^T \\mathbf{w} \\mathbf{s}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f566de67-579a-43be-a89e-18752483a567",
   "metadata": {},
   "source": [
    "Then we substitute it to the above equation. $$||R\\mathbf{x}- \\mathbf{s}||^2_\\mathbf{w} = (R\\mathbf{x}- \\mathbf{s})^T \\mathbf{w} (R\\mathbf{x}- \\mathbf{s}) \\\\\n",
    "  = \\mathbf{x}^T R^T \\mathbf{w} R\\mathbf{x} - \\mathbf{x}^T R^T \\mathbf{w} \\mathbf{s} - \\mathbf{x}^T R^T \\mathbf{w} \\mathbf{s} \\\\ = \\mathbf{x}^T R^T \\mathbf{w} R\\mathbf{x} - 2\\mathbf{x}^T R^T \\mathbf{w} \\mathbf{s}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68cb59dc-e4f6-4486-a5aa-19a0a573a126",
   "metadata": {},
   "source": [
    "Since we discarded a constant term, the equation no longer equals but will have proportional relations. In addition, the property of a double transpose equals to itself can be applied to the $\\mathbf{x}^T R^T \\mathbf{w} \\mathbf{s}$, go back to the equation and we have:\n",
    "\n",
    "$$||R\\mathbf{x}- \\mathbf{s}||^2_\\mathbf{w} \\propto \\mathbf{x}^T R^T \\mathbf{w} R\\mathbf{x} - 2\\mathbf{x} (R^T \\mathbf{w}\\mathbf{s})^T$$\n",
    "\n",
    "Thus, we divide both sides by a factor of 2 and make the format looks like a standard form of quadratic optimization problem:\n",
    "\n",
    "$$\\frac{1}{2} ||R\\mathbf{x}- \\mathbf{s}||^2_\\mathbf{w} \\propto \\frac{1}{2} \\mathbf{x}^T R^T \\mathbf{w} R\\mathbf{x} - \\mathbf{x} (R^T \\mathbf{w}\\mathbf{s})^T$$\n",
    "\n",
    "We can use P and q to represent the parts in this formula:\n",
    "\n",
    "$$\\frac{1}{2} ||R\\mathbf{x}- \\mathbf{s}||^2_\\mathbf{w} \\propto \\frac{1}{2}\\mathbf{x}^T P \\mathbf{x} +\\mathbf{q}^T \\mathbf{x}$$\n",
    "\n",
    "where\n",
    "$$P=R^T \\mathbf{w} R$$ and $$q=-R^T \\mathbf{w}\\mathbf{s}$$\n",
    "\n",
    "Now we successfully convert a least-square method to a quadratic optimization. This helps in situations when least-square cannot do expressive work in complicated optimization problems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe25030-8afd-49e1-b1de-a8c0fc3b0ea2",
   "metadata": {},
   "source": [
    "## Quadratic Optimization in Potrfolio Selection\n",
    "\n",
    "### Intuition\n",
    "\n",
    "Least-Square method can be applied to financial assets to minimize transcation costs and get the relationship between two ascepts, for instance the stock price per share and it's expected rate of returns. The previous section suggests that we can convert least-square method to quadratic optimization in financial investment. Thus, Quadratic optimization could also be applied to financial problems when investors buy mutual financial products at the same time. Usually, we combined stocks, mutual funds and banking products together and calculate proper allocations of buying these bonds under certain rate of returns. However, the coefficients of the objective functions and constraints can be non-integers. If we calculate the quadratic optimization by hand, it will be not efficient. Thus, we can perform the calculation with quadratic problem solvers.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "607c25c8-b49d-4967-9303-77d97a562d50",
   "metadata": {},
   "source": [
    "### Case Study\n",
    "Here consider a problem of financial protfolio optimzation for a strategy of purchasing a bundle of US stocks, bonds and cash. Unlike traditional real-world problems, financial returns can be calculated by estimating covariances between each financial products. If we recall the previous section of least-square method, we have denoted that matrix $\\mathbf{w}$ a weight matrix with covariances between each decision variable. I tried an example in the book written by Cornuejols and Tütüncü, section 8.4. Basically it provides with a standard financial problem and I am going to solve this quadratic optimization problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04ce8bf5-6759-4b78-af56-f88c4854435b",
   "metadata": {},
   "source": [
    "Let's say if we want to find out the rate of return from 4.0% to 10.5% with increments of 0.5%, and we have:\n",
    "\n",
    "$$\n",
    "\\begin{array}{rc}\n",
    "\\text{minimize:} & 0.02778x^2_S+2*0.00387x_Sx_B+2*0.00021x_Sx_C+0.01112x^2_B-2*0.00020x_Bx_C+0.00115x^2_C\\\\\n",
    "\\text{subject to:} & 0.1177x_S 0.0751x_B 0.0234x_C \\geq R \\\\\n",
    "& x_S+x_B+x_C = 1 \\\\\n",
    "& x_S,x_B,x_C \\geq 0\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "$x_S,x_B,x_C$ correspond to stocks, bonds and cash respectively. The coefficients on the objective function are the covariances of three types of products. The R denotes to the rate of return. Now we want to use qp solver to solve this quadratic optimization problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "605be4db-538c-44a5-afa1-aa62b6034c15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting qpsolvers[open_source_solvers]\n",
      "  Using cached qpsolvers-2.6.0-py3-none-any.whl (50 kB)\n",
      "Requirement already satisfied: numpy>=1.15.4 in /opt/conda/lib/python3.10/site-packages (from qpsolvers[open_source_solvers]) (1.22.4)\n",
      "Requirement already satisfied: scipy>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from qpsolvers[open_source_solvers]) (1.9.1)\n",
      "Collecting scs>=3.0.1\n",
      "  Using cached scs-3.2.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.7 MB)\n",
      "Collecting osqp>=0.6.2\n",
      "  Using cached osqp-0.6.2.post5-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (298 kB)\n",
      "Requirement already satisfied: cvxopt>=1.2.6 in /opt/conda/lib/python3.10/site-packages (from qpsolvers[open_source_solvers]) (1.3.0)\n",
      "Collecting proxsuite>=0.2.2\n",
      "  Using cached proxsuite-0.2.10-0-cp310-cp310-manylinux_2_31_x86_64.whl (1.6 MB)\n",
      "Collecting ecos>=2.0.8\n",
      "  Using cached ecos-2.0.10-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (190 kB)\n",
      "Collecting qdldl\n",
      "  Using cached qdldl-0.1.5.post2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
      "Collecting cmeel\n",
      "  Downloading cmeel-0.22.0-py3-none-any.whl (10 kB)\n",
      "Requirement already satisfied: tomli<3.0.0,>=2.0.1 in /opt/conda/lib/python3.10/site-packages (from cmeel->proxsuite>=0.2.2->qpsolvers[open_source_solvers]) (2.0.1)\n",
      "Installing collected packages: cmeel, scs, qpsolvers, qdldl, proxsuite, ecos, osqp\n",
      "Successfully installed cmeel-0.22.0 ecos-2.0.10 osqp-0.6.2.post5 proxsuite-0.2.10 qdldl-0.1.5.post2 qpsolvers-2.6.0 scs-3.2.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install qpsolvers[open_source_solvers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8eb232b5-8cd6-48b1-ad7e-3a1099f06ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from qpsolvers import solve_qp\n",
    "from numpy import array, dot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6646fa4e-4e8d-4cc5-940f-6d8b29ae0408",
   "metadata": {},
   "source": [
    "We import solve_qp from qpsolvers, in the qpsolvers, there are multiple solvers for quadratic programs, we use solver \"osqp\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c25470d7-05de-4313-bca5-caa0c1404655",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define a function to test whether the matrix P is a positive definte matrix; return True if it is.\n",
    "def is_pos_def(x):\n",
    "    return np.all(np.linalg.eigvals(x) > 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfcff78c-092b-4837-a734-8f6f5e63f06a",
   "metadata": {},
   "source": [
    "Solver can be wrong if the matrix P is not a positive definite matrix, thus we define a function to prevent errors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ad30e1-42ec-405e-a890-1400211c4773",
   "metadata": {},
   "source": [
    "#### Solve by Solver 'osqp'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cf48d5a6-4327-432a-8def-6f3303724e4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QP solution: x = [0.07 0.18 0.74]\n",
      "QP solution: x = [0.11 0.21 0.67]\n",
      "QP solution: x = [0.15 0.25 0.6 ]\n",
      "QP solution: x = [0.18 0.28 0.54]\n",
      "QP solution: x = [0.21 0.32 0.47]\n",
      "QP solution: x = [0.25 0.35 0.4 ]\n",
      "QP solution: x = [0.28 0.39 0.33]\n",
      "QP solution: x = [0.31 0.42 0.27]\n",
      "QP solution: x = [0.35 0.45 0.2 ]\n",
      "QP solution: x = [0.38 0.49 0.13]\n",
      "QP solution: x = [0.42 0.52 0.06]\n",
      "QP solution: x = [0.46 0.54 0.  ]\n",
      "QP solution: x = [0.59 0.41 0.  ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/qpsolvers/solvers/conversions/warnings.py:34: UserWarning: Converted P to scipy.sparse.csc.csc_matrix\n",
      "For best performance, build P as a scipy.sparse.csc_matrix rather than as a numpy.ndarray\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/qpsolvers/solvers/conversions/warnings.py:34: UserWarning: Converted G to scipy.sparse.csc.csc_matrix\n",
      "For best performance, build G as a scipy.sparse.csc_matrix rather than as a numpy.ndarray\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/qpsolvers/solvers/conversions/warnings.py:34: UserWarning: Converted A to scipy.sparse.csc.csc_matrix\n",
      "For best performance, build A as a scipy.sparse.csc_matrix rather than as a numpy.ndarray\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## We create matrices to solve using osqp solver to solve the quadratic problem. The for loop starts when the return\n",
    "## rate equals to 4.0% to 11.5%, increases by 0.5% each time.\n",
    "for R in np.arange(0.040,0.105,0.005):    \n",
    "    h = np.array([R])\n",
    "    P = np.matrix([[0.02778,0.00387,0.00021],\n",
    "              [0.00387,0.01112,-0.00020],\n",
    "              [0.00021,-0.00020,0.00115]])\n",
    "    q = array([0.,0.,0.]) ## We do not have q in this example so we put 0\n",
    "    G = np.array([0.1177, 0.0751, 0.0234])\n",
    "    A = np.matrix([1.,1.,1.])\n",
    "    b = np.array([1.])\n",
    "    lb = np.array([0.,0.,0.])\n",
    "    x = (solve_qp(P, q, -G, -h, A, b,lb, solver=\"osqp\").round(2))\n",
    "    print(\"QP solution: x = {}\".format(x))\n",
    "is_pos_def(P) ## produce true if P matrix is a positively definte matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7991579c-5395-417b-a5e1-aaad6c7e51b9",
   "metadata": {},
   "source": [
    "We fill in all the components in this quadratic optimization and start the solver. R is the expected rates of return from 4.0% to 10.5% so that we make this into a for loop for computational efficiency. The results show the weights of three assets given different rates of return. For example, if we expect the rate of return equals to 4.0%, then we should invest 7% of the total money on stocks, 18% of the total money on bonds and 74% of the total money on cash.\n",
    "\n",
    "From the results of QP solution, we retrieved the best allocation of capacity under different values of rate of return. If we sum the results in each x array, we get the total capacity of 1. Since financial investments are divided into portions, we can have a better interpretation of which types of product should we focus on the most. As the result suggests, when we expect a lower rate of return, we can see that the proportion to store cash is very high, while the proportion of investing stocks are low. However, if we seek for higher rates of return, the proportion to invest bonds and stocks should be high and finally we do not need to store cash anymore.\n",
    "\n",
    "Sometimes the decision variables can be a lot of we specify certain stocks and bonds. With the help of a solver, we can get the final answer efficiently."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49deb410-d893-4019-925e-aa32db7ce089",
   "metadata": {},
   "source": [
    "#### Solve by Solvers 'cvxopt' and 'ecos'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "024761c3-e567-4cc9-8343-1465faf56464",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QP solution: x = [0.08 0.17 0.75]\n",
      "QP solution: x = [0.12 0.21 0.68]\n",
      "QP solution: x = [0.15 0.24 0.61]\n",
      "QP solution: x = [0.18 0.28 0.54]\n",
      "QP solution: x = [0.22 0.31 0.47]\n",
      "QP solution: x = [0.25 0.35 0.4 ]\n",
      "QP solution: x = [0.28 0.39 0.33]\n",
      "QP solution: x = [0.32 0.42 0.26]\n",
      "QP solution: x = [0.35 0.46 0.19]\n",
      "QP solution: x = [0.38 0.49 0.12]\n",
      "QP solution: x = [0.42 0.53 0.05]\n",
      "QP solution: x = [0.47 0.53 0.  ]\n",
      "QP solution: x = [0.58 0.42 0.  ]\n"
     ]
    }
   ],
   "source": [
    "for R in np.arange(0.040,0.105,0.005):    \n",
    "    h = np.array([R])\n",
    "    P = np.matrix([[0.02778,0.00387,0.00021],\n",
    "              [0.00387,0.01112,-0.00020],\n",
    "              [0.00021,-0.00020,0.00115]])\n",
    "    q = array([0.,0.,0.]) ## We do not have q in this example so we put 0\n",
    "    G = np.array([0.1177, 0.0751, 0.0234])\n",
    "    A = np.matrix([1.,1.,1.])\n",
    "    b = np.array([1.])\n",
    "    lb = np.array([0.,0.,0.])\n",
    "    x = (solve_qp(P, q, -G, -h, A, b,lb, solver=\"cvxopt\").round(2))\n",
    "    print(\"QP solution: x = {}\".format(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "54e1041f-b9f1-46de-807c-363f94fedd24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QP solution: x = [0.08 0.17 0.75]\n",
      "QP solution: x = [0.12 0.21 0.68]\n",
      "QP solution: x = [0.15 0.24 0.61]\n",
      "QP solution: x = [0.18 0.28 0.54]\n",
      "QP solution: x = [0.22 0.31 0.47]\n",
      "QP solution: x = [0.25 0.35 0.4 ]\n",
      "QP solution: x = [0.28 0.39 0.33]\n",
      "QP solution: x = [0.32 0.42 0.26]\n",
      "QP solution: x = [0.35 0.46 0.19]\n",
      "QP solution: x = [0.38 0.49 0.12]\n",
      "QP solution: x = [0.42 0.53 0.05]\n",
      "QP solution: x = [0.47 0.53 0.  ]\n",
      "QP solution: x = [0.58 0.42 0.  ]\n"
     ]
    }
   ],
   "source": [
    "for R in np.arange(0.040,0.105,0.005):    \n",
    "    h = np.array([R])\n",
    "    P = np.matrix([[0.02778,0.00387,0.00021],\n",
    "              [0.00387,0.01112,-0.00020],\n",
    "              [0.00021,-0.00020,0.00115]])\n",
    "    q = array([0.,0.,0.]) ## We do not have q in this example so we put 0\n",
    "    G = np.array([0.1177, 0.0751, 0.0234])\n",
    "    A = np.matrix([1.,1.,1.])\n",
    "    b = np.array([1.])\n",
    "    lb = np.array([0.,0.,0.])\n",
    "    x = (solve_qp(P, q, -G, -h, A, b,lb, solver=\"ecos\").round(2))\n",
    "    print(\"QP solution: x = {}\".format(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b63a09d5-2dbb-480d-b0d1-d2c421a9fa78",
   "metadata": {},
   "source": [
    "Different from the solver 'osqp', solvers 'cvxopt' and 'ecos' give slightly different answers to the quadratic optimization. The reason could be the different calculation methods of solvers. When I viewed the solver webpage https://pypi.org/project/qpsolvers/, the algorithm of 'cvxopt' and 'ecos' is interior point algorithm and the algorithm of 'osqp' is augmented Lagrangian. For other quadratic problems, we can also compare the results of solvers using different calculation algorithm and check if the results are accurate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3585dbf-4ceb-4608-b606-313eb6ce9340",
   "metadata": {},
   "source": [
    "## Conslusion\n",
    "\n",
    "Quadratic optimization require more calculations than linear optimization and often have applications related with least-square method, especially in financial investments. As least square method has close connections with quadratic optimization, we can convert some of the problems to quadratic optimization for further calculations. However, least square method is not exactly equivalent to quadratic optimization, and we have to be check the P matrix and q vector before doing the calculations. Also, the quadratic programming solvers can help with complicated questions with various calculational algorithms. We can increase the accuracy of the results if we put the problem into multiple solvers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7866d1c-2dcc-4357-b693-3d72597236e3",
   "metadata": {},
   "source": [
    "## Reference\n",
    "\n",
    "Caron, S. (n.d.). Conversion from least squares to quadratic programming. Stphane Carons Atom. Retrieved December 8, 2022, from https://scaron.info/blog/conversion-from-least-squares-to-quadratic-programming.html \n",
    "\n",
    "Cornuejols, G., & Tütüncü, R. (2006). Optimization methods in finance (Vol. 5). Cambridge University Press.\n",
    "\n",
    "Qpsolvers. PyPI. (n.d.). Retrieved December 8, 2022, from https://pypi.org/project/qpsolvers/ \n",
    "\n",
    "Wikimedia Foundation. (2022, November 26). Quadratic Programming. Wikipedia. Retrieved December 8, 2022, from https://en.wikipedia.org/wiki/Quadratic_programming "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
